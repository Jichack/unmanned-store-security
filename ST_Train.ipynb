{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNTa3R+na1DFhhP6tIjCpC4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iXe3IWaXYStL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2c8d54f-26ba-493a-c7b8-dbd2a025a382"},"outputs":[{"output_type":"stream","name":"stdout","text":["â–¶ ë°ì´í„° ë¡œë”©...\n","\n","ğŸš€ ì´ 5íšŒ ë°˜ë³µ ì‹¤í—˜ ì‹œì‘ (ëª¨ë“  ì§€í‘œ ìˆ˜ì§‘)\n","------------------------------------------------------------\n","ğŸ”„ Run 1/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 1/30 | Train Loss: 0.8605 | Val Acc: 49.66%\n","      â””â”€â”€ Best Accuracy Updated! (49.66%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 2/30 | Train Loss: 0.6773 | Val Acc: 63.21%\n","      â””â”€â”€ Best Accuracy Updated! (63.21%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 3/30 | Train Loss: 0.5863 | Val Acc: 75.55%\n","      â””â”€â”€ Best Accuracy Updated! (75.55%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 4/30 | Train Loss: 0.5351 | Val Acc: 76.62%\n","      â””â”€â”€ Best Accuracy Updated! (76.62%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 5/30 | Train Loss: 0.4933 | Val Acc: 70.81%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 6/30 | Train Loss: 0.4764 | Val Acc: 73.65%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 7/30 | Train Loss: 0.4616 | Val Acc: 78.38%\n","      â””â”€â”€ Best Accuracy Updated! (78.38%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 8/30 | Train Loss: 0.4425 | Val Acc: 80.78%\n","      â””â”€â”€ Best Accuracy Updated! (80.78%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 9/30 | Train Loss: 0.4302 | Val Acc: 72.22%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 10/30 | Train Loss: 0.4191 | Val Acc: 82.16%\n","      â””â”€â”€ Best Accuracy Updated! (82.16%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 11/30 | Train Loss: 0.4102 | Val Acc: 75.77%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 12/30 | Train Loss: 0.4033 | Val Acc: 82.96%\n","      â””â”€â”€ Best Accuracy Updated! (82.96%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 13/30 | Train Loss: 0.3941 | Val Acc: 79.88%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 14/30 | Train Loss: 0.3883 | Val Acc: 81.09%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 15/30 | Train Loss: 0.3855 | Val Acc: 68.27%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 16/30 | Train Loss: 0.3342 | Val Acc: 85.62%\n","      â””â”€â”€ Best Accuracy Updated! (85.62%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 17/30 | Train Loss: 0.3234 | Val Acc: 84.27%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 18/30 | Train Loss: 0.3113 | Val Acc: 84.30%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 19/30 | Train Loss: 0.3130 | Val Acc: 84.25%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 20/30 | Train Loss: 0.3077 | Val Acc: 79.82%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 21/30 | Train Loss: 0.3031 | Val Acc: 85.37%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 22/30 | Train Loss: 0.3014 | Val Acc: 77.58%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 23/30 | Train Loss: 0.3001 | Val Acc: 85.56%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 24/30 | Train Loss: 0.2955 | Val Acc: 85.79%\n","      â””â”€â”€ Best Accuracy Updated! (85.79%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 25/30 | Train Loss: 0.2991 | Val Acc: 86.43%\n","      â””â”€â”€ Best Accuracy Updated! (86.43%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 26/30 | Train Loss: 0.2965 | Val Acc: 85.91%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 27/30 | Train Loss: 0.2932 | Val Acc: 86.63%\n","      â””â”€â”€ Best Accuracy Updated! (86.63%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 28/30 | Train Loss: 0.2930 | Val Acc: 79.88%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 29/30 | Train Loss: 0.2912 | Val Acc: 84.99%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 1] Epoch 30/30 | Train Loss: 0.2934 | Val Acc: 87.06%\n","      â””â”€â”€ Best Accuracy Updated! (87.06%)\n","------------------------------------------------------------\n","ğŸ”„ Run 2/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 2] Epoch 1/30 | Train Loss: 0.8738 | Val Acc: 72.01%\n","      â””â”€â”€ Best Accuracy Updated! (72.01%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 2] Epoch 2/30 | Train Loss: 0.6340 | Val Acc: 80.38%\n","      â””â”€â”€ Best Accuracy Updated! (80.38%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 2] Epoch 3/30 | Train Loss: 0.5493 | Val Acc: 78.93%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 2] Epoch 4/30 | Train Loss: 0.5051 | Val Acc: 60.10%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 2] Epoch 5/30 | Train Loss: 0.4773 | Val Acc: 84.49%\n","      â””â”€â”€ Best Accuracy Updated! (84.49%)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 2] Epoch 6/30 | Train Loss: 0.4493 | Val Acc: 81.26%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 2] Epoch 7/30 | Train Loss: 0.4426 | Val Acc: 75.07%\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["   [Run 2] Epoch 8/30 | Train Loss: 0.4218 | Val Acc: 84.40%\n"]},{"output_type":"stream","name":"stderr","text":[]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sns\n","import os\n","from google.colab import drive\n","from tqdm import tqdm\n","import collections\n","\n","# === 1. ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼) ===\n","if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","ROOT_PATH = '/content/drive/MyDrive/Capstone_Project'\n","DATA_PATH = os.path.join(ROOT_PATH, 'processed_data', 'train_data_30_10.npy')\n","LABEL_PATH = os.path.join(ROOT_PATH, 'processed_data', 'train_label_30_10.npy')\n","MODEL_SAVE_BASE = os.path.join(ROOT_PATH, 'experiment_full_metric')\n","\n","CLASSES = ['Walking', 'Shopping', 'Falldown', 'Threat']\n","BATCH_SIZE = 32\n","EPOCHS = 30\n","LEARNING_RATE = 0.01\n","NUM_RUNS = 5\n","step_size = 15\n","\n","\n","# === 2. ë°ì´í„° ë¡œë“œ ===\n","print(\"â–¶ ë°ì´í„° ë¡œë”©...\")\n","X = np.load(DATA_PATH)\n","Y = np.load(LABEL_PATH)\n","\n","class_weights = compute_class_weight('balanced', classes=np.unique(Y), y=Y)\n","class_weights = torch.tensor(class_weights, dtype=torch.float32)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# === 3. ëª¨ë¸ ì •ì˜ (ì—¬ê¸°ì— ST-GCN ë˜ëŠ” LSTM í´ë˜ìŠ¤ ì½”ë“œ ë°˜ë“œì‹œ í¬í•¨!) ===\n","class Graph:\n","    def __init__(self, num_node=17):\n","        self.num_node = num_node\n","        self.edges = [(0,1),(0,2),(1,3),(2,4),(5,7),(7,9),(6,8),(8,10),(5,6),(5,11),(6,12),(11,12),(11,13),(13,15),(12,14),(14,16)]\n","        self.A = self.get_adjacency_matrix(self.edges, self.num_node)\n","    def get_adjacency_matrix(self, edges, num_node):\n","        A = np.zeros((num_node, num_node))\n","        for i in range(num_node): A[i, i] = 1\n","        for edge in edges:\n","            A[edge[0], edge[1]] = 1\n","            A[edge[1], edge[0]] = 1\n","        row_sum = np.sum(A, axis=1)\n","        D_inv_sqrt = np.power(row_sum, -0.5)\n","        D_inv_sqrt[np.isinf(D_inv_sqrt)] = 0.\n","        D_mat = np.diag(D_inv_sqrt)\n","        return torch.tensor(D_mat.dot(A).dot(D_mat), dtype=torch.float32)\n","\n","class STGCNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, A, stride=1, residual=True):\n","        super().__init__()\n","        self.A = A\n","        self.gcn = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        self.tcn = nn.Sequential(\n","            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n","            # Kernel size (9,1)ì€ Tê°€ 9ë³´ë‹¤ ì‘ìœ¼ë©´ ì—ëŸ¬ë‚  ìˆ˜ ìˆìœ¼ë‚˜ padding=4ê°€ ìˆì–´ì„œ T=1ë„ ì²˜ë¦¬ê°€ëŠ¥\n","            nn.Conv2d(out_channels, out_channels, kernel_size=(9,1), padding=(4,0), stride=(stride,1)),\n","            nn.BatchNorm2d(out_channels), nn.Dropout(0.5, inplace=True)\n","        )\n","        if not residual: self.residual_conv = lambda x: 0\n","        elif (in_channels == out_channels) and (stride == 1): self.residual_conv = lambda x: x\n","        else: self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(stride,1))\n","    def forward(self, x):\n","        x_spatial = self.gcn(x)\n","        N, C, T, V = x_spatial.size()\n","        x_spatial = torch.matmul(x_spatial.view(N, C*T, V), self.A.to(x.device)).view(N, C, T, V)\n","        return torch.relu(self.tcn(x_spatial) + self.residual_conv(x))\n","\n","class ActionRecognitionModel(nn.Module):\n","    def __init__(self, num_classes, in_channels=3, num_frames=30, num_joints=17):\n","        super().__init__()\n","        graph = Graph(num_node=num_joints)\n","        self.A = nn.Parameter(graph.A, requires_grad=False)\n","        self.data_bn = nn.BatchNorm1d(in_channels * num_joints)\n","\n","        # [ìˆ˜ì •ë¨ 3] ê²½ëŸ‰í™” ì±„ë„ ì ìš© (Half-STGCN)\n","        # 64-128-256 êµ¬ì¡°ë¥¼ -> 32-64-128 êµ¬ì¡°ë¡œ ë³€ê²½í•˜ì—¬ íŒŒë¼ë¯¸í„°ì™€ ì—°ì‚°ëŸ‰ ê°ì†Œ\n","        self.st_gcn_networks = nn.Sequential(\n","            STGCNBlock(in_channels, 32, self.A),  # 64 -> 32\n","            STGCNBlock(32, 32, self.A),           # 64 -> 32\n","            STGCNBlock(32, 32, self.A),           # 64 -> 32\n","            STGCNBlock(32, 64, self.A, stride=2), # 64 -> 32, 128 -> 64\n","            STGCNBlock(64, 64, self.A),           # 128 -> 64\n","            STGCNBlock(64, 64, self.A),           # 128 -> 64\n","            STGCNBlock(64, 128, self.A, stride=2),# 128 -> 64, 256 -> 128\n","            STGCNBlock(128, 128, self.A),         # 256 -> 128\n","            STGCNBlock(128, 128, self.A)          # 256 -> 128\n","        )\n","        # ë§ˆì§€ë§‰ FC Layer ì…ë ¥ ì±„ë„ë„ 128ë¡œ ê°ì†Œ\n","        self.fcn = nn.Conv2d(128, num_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","        N, C, T, V, M = x.size()\n","        x = x.mean(dim=4) # (N, C, T, V)\n","        x = x.permute(0, 1, 3, 2).contiguous().view(N, C*V, T)\n","        x = self.data_bn(x)\n","        x = x.view(N, C, V, T).permute(0, 1, 3, 2).contiguous()\n","        x = self.st_gcn_networks(x)\n","        # Global Average Pooling (T, V ì°¨ì›ì— ëŒ€í•´ í‰ê· )\n","        x = torch.nn.functional.avg_pool2d(x, x.size()[2:])\n","        return self.fcn(x).view(x.size(0), -1)\n","\n","# class Graph:\n","#     def __init__(self, num_node=17):\n","#         self.num_node = num_node\n","#         self.edges = [(0,1),(0,2),(1,3),(2,4),(5,7),(7,9),(6,8),(8,10),(5,6),(5,11),(6,12),(11,12),(11,13),(13,15),(12,14),(14,16)]\n","#         self.A = self.get_adjacency_matrix(self.edges, self.num_node)\n","#     def get_adjacency_matrix(self, edges, num_node):\n","#         A = np.zeros((num_node, num_node))\n","#         for i in range(num_node): A[i, i] = 1\n","#         for edge in edges:\n","#             A[edge[0], edge[1]] = 1\n","#             A[edge[1], edge[0]] = 1\n","#         row_sum = np.sum(A, axis=1)\n","#         D_inv_sqrt = np.power(row_sum, -0.5)\n","#         D_inv_sqrt[np.isinf(D_inv_sqrt)] = 0.\n","#         D_mat = np.diag(D_inv_sqrt)\n","#         return torch.tensor(D_mat.dot(A).dot(D_mat), dtype=torch.float32)\n","\n","# class STGCNBlock(nn.Module):\n","#     def __init__(self, in_channels, out_channels, A, stride=1, residual=True):\n","#         super().__init__()\n","#         self.A = A\n","#         self.gcn = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","#         self.tcn = nn.Sequential(\n","#             nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n","#             nn.Conv2d(out_channels, out_channels, kernel_size=(9,1), padding=(4,0), stride=(stride,1)),\n","#             nn.BatchNorm2d(out_channels), nn.Dropout(0.5, inplace=True)\n","#         )\n","#         if not residual: self.residual_conv = lambda x: 0\n","#         elif (in_channels == out_channels) and (stride == 1): self.residual_conv = lambda x: x\n","#         else: self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(stride,1))\n","#     def forward(self, x):\n","#         x_spatial = self.gcn(x)\n","#         N, C, T, V = x_spatial.size()\n","#         x_spatial = torch.matmul(x_spatial.view(N, C*T, V), self.A.to(x.device)).view(N, C, T, V)\n","#         return torch.relu(self.tcn(x_spatial) + self.residual_conv(x))\n","\n","# class ActionRecognitionModel(nn.Module):\n","#     def __init__(self, num_classes, in_channels=3, num_frames=30, num_joints=17):\n","#         super().__init__()\n","#         graph = Graph(num_node=num_joints)\n","#         self.A = nn.Parameter(graph.A, requires_grad=False)\n","#         self.data_bn = nn.BatchNorm1d(in_channels * num_joints)\n","#         self.st_gcn_networks = nn.Sequential(\n","#             STGCNBlock(in_channels, 64, self.A), STGCNBlock(64, 64, self.A), STGCNBlock(64, 64, self.A),\n","#             STGCNBlock(64, 128, self.A, stride=2), STGCNBlock(128, 128, self.A), STGCNBlock(128, 128, self.A),\n","#             STGCNBlock(128, 256, self.A, stride=2), STGCNBlock(256, 256, self.A), STGCNBlock(256, 256, self.A)\n","#         )\n","#         self.fcn = nn.Conv2d(256, num_classes, kernel_size=1)\n","#     def forward(self, x):\n","#         N, C, T, V, M = x.size()\n","#         x = x.mean(dim=4)\n","#         x = x.permute(0, 1, 3, 2).contiguous().view(N, C*V, T)\n","#         x = self.data_bn(x)\n","#         x = x.view(N, C, V, T).permute(0, 1, 3, 2).contiguous()\n","#         x = self.st_gcn_networks(x)\n","#         x = torch.nn.functional.avg_pool2d(x, x.size()[2:])\n","#         return self.fcn(x).view(x.size(0), -1)\n","\n","#[Option 2] LSTM (ë¹„êµ ëª¨ë¸) ì‹¤í—˜ ì‹œ ì‚¬ìš© (ì´ê±¸ë¡œ êµì²´!)\n","# === 3. ëª¨ë¸ ì •ì˜ (Baseline LSTM) ===\n","# class BaselineLSTM(nn.Module):\n","#     def __init__(self, num_classes, input_size, hidden_size=128, num_layers=2):\n","#         super(BaselineLSTM, self).__init__()\n","#         # batch_first=True: ì…ë ¥ì´ (Batch, Time, Feature) í˜•íƒœì„\n","#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n","#         self.fc = nn.Linear(hidden_size, num_classes)\n","\n","#     def forward(self, x):\n","#         # ì…ë ¥ x í˜•íƒœ: (N, C, T, V, M) -> 5ì°¨ì› í…ì„œ\n","#         N, C, T, V, M = x.size()\n","\n","#         # 1. ì°¨ì› ë³€ê²½: (N, T, C, V, M) ìˆœì„œë¡œ ë°”ê¿ˆ (ì‹œê°„ ì¶•ì„ ì•ìœ¼ë¡œ)\n","#         x = x.permute(0, 2, 1, 3, 4).contiguous()\n","\n","#         # 2. Flatten: (N, T, C*V*M) -> (N, T, 51)\n","#         # ê³µê°„ì  êµ¬ì¡°(Graph)ë¥¼ ë¬´ì‹œí•˜ê³  í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í¼ì¹¨ -> ì´ê²ƒì´ LSTMì˜ í•œê³„ì  ë…¼ë¦¬!\n","#         x = x.view(N, T, -1)\n","\n","#         # 3. LSTM í†µê³¼\n","#         # out: (Batch, Time, Hidden)\n","#         # hidden: (Layers, Batch, Hidden)\n","#         lstm_out, (hidden, cell) = self.lstm(x)\n","\n","#         # 4. ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜\n","#         # Many-to-One êµ¬ì¡°\n","#         last_output = lstm_out[:, -1, :]\n","#         return self.fc(last_output)\n","\n","\n","# === 4. ê²°ê³¼ ì €ì¥ì†Œ ì´ˆê¸°í™” ===\n","# ê° ì§€í‘œë³„ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ ì €ì¥ (Key: Class -> Metric -> List of values)\n","metrics_storage = {\n","    cls: {'precision': [], 'recall': [], 'f1-score': []}\n","    for cls in CLASSES\n","}\n","metrics_storage['Overall'] = {'accuracy': []} # ì „ì²´ ì •í™•ë„ ë”°ë¡œ ì €ì¥\n","\n","print(f\"\\nğŸš€ ì´ {NUM_RUNS}íšŒ ë°˜ë³µ ì‹¤í—˜ ì‹œì‘ (ëª¨ë“  ì§€í‘œ ìˆ˜ì§‘)\")\n","\n","for run in range(1, NUM_RUNS + 1):\n","    print(\"-\" * 60)\n","    print(f\"ğŸ”„ Run {run}/{NUM_RUNS}\")\n","\n","    # ë°ì´í„° ë¶„í• \n","    X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=None, stratify=Y)\n","    train_data = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n","    val_data = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n","    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n","    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    # ëª¨ë¸ ì´ˆê¸°í™” (â˜… ëª¨ë¸ëª… í™•ì¸!)\n","    model = ActionRecognitionModel(num_classes=len(CLASSES)).to(device)\n","    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1)\n","\n","    best_acc = 0.0\n","    best_model_path = f\"{MODEL_SAVE_BASE}_{run}.pth\"\n","\n","    # í•™ìŠµ\n","    for epoch in range(EPOCHS):\n","        # 1. Training\n","        model.train()\n","        running_loss = 0.0 # Loss ëˆ„ì  ë³€ìˆ˜ ì´ˆê¸°í™”\n","\n","        # tqdmìœ¼ë¡œ ì§„í–‰ë°” í‘œì‹œ (leave=Falseë¡œ ì„¤ì •í•˜ì—¬ ì™„ë£Œ í›„ ì¤„ë°”ê¿ˆ ë°©ì§€ ê¹”ë”í•˜ê²Œ)\n","        train_loop = tqdm(train_loader, desc=f\"   Epoch {epoch+1}/{EPOCHS}\", leave=False)\n","\n","        for inputs, labels in train_loop:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            train_loop.set_postfix(loss=loss.item()) # ì§„í–‰ë°” ì˜†ì— ì‹¤ì‹œê°„ Loss í‘œì‹œ\n","\n","        # Epoch í‰ê·  Loss ê³„ì‚°\n","        avg_train_loss = running_loss / len(train_loader)\n","\n","        # 2. Validation\n","        model.eval()\n","        correct, total = 0, 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        acc = 100 * correct / total\n","        scheduler.step()\n","\n","        # â˜… [ì¶”ê°€ëœ ë¶€ë¶„] ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥ (Loss ë° Acc í™•ì¸)\n","        print(f\"   [Run {run}] Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Acc: {acc:.2f}%\")\n","\n","        if acc > best_acc:\n","            best_acc = acc\n","            torch.save(model.state_dict(), best_model_path)\n","            # ìµœê³  ê¸°ë¡ ê°±ì‹  ì‹œ ì˜†ì— í‘œì‹œí•´ì£¼ë©´ ì¢‹ìŒ\n","            print(f\"      â””â”€â”€ Best Accuracy Updated! ({best_acc:.2f}%)\")\n","\n","# í‰ê°€ (Best Model ë¡œë“œ)\n","print(f\"\\n   âœ… Run {run} Finished. Final Best Acc: {best_acc:.2f}%\")\n","\n","model.load_state_dict(torch.load(best_model_path))\n","model.eval()\n","all_preds, all_labels = [], []\n","with torch.no_grad():\n","    for inputs, labels in val_loader:\n","        inputs = inputs.to(device)\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.numpy())\n","\n","# Classification Report ì €ì¥\n","report = classification_report(all_labels, all_preds, target_names=CLASSES, output_dict=True)\n","\n","metrics_storage['Overall']['accuracy'].append(report['accuracy'])\n","for cls in CLASSES:\n","        metrics_storage[cls]['precision'].append(report[cls]['precision'])\n","        metrics_storage[cls]['recall'].append(report[cls]['recall'])\n","        metrics_storage[cls]['f1-score'].append(report[cls]['f1-score'])\n","\n","print(f\"   âœ… Run {run} Best Acc: {best_acc:.2f}% | Fall Recall: {report['Falldown']['recall']:.4f}\")\n","\n","# === 5. ìµœì¢… ì¢…í•© ë¦¬í¬íŠ¸ ì¶œë ¥ ===\n","print(\"\\n\" + \"=\" * 80)\n","print(f\"ğŸ“Š {NUM_RUNS}íšŒ ë°˜ë³µ ì‹¤í—˜ ì¢…í•© ê²°ê³¼ (Mean Â± Std)\")\n","print(\"=\" * 80)\n","\n","# 1. ì „ì²´ ì •í™•ë„\n","acc_mean = np.mean(metrics_storage['Overall']['accuracy']) * 100\n","acc_std = np.std(metrics_storage['Overall']['accuracy']) * 100\n","print(f\"ğŸ† Overall Accuracy: {acc_mean:.2f} Â± {acc_std:.2f}%\")\n","print(\"-\" * 80)\n","\n","# 2. í´ë˜ìŠ¤ë³„ ìƒì„¸ í…Œì´ë¸”\n","header = f\"{'Class':<12} | {'Metric':<10} | {'Mean':<8} | {'Std':<8}\"\n","print(header)\n","print(\"-\" * 45)\n","\n","for cls in CLASSES:\n","    for metric in ['precision', 'recall', 'f1-score']:\n","        values = metrics_storage[cls][metric]\n","        mean_val = np.mean(values)\n","        std_val = np.std(values)\n","\n","        # ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n","        print(f\"{cls:<12} | {metric.capitalize():<10} | {mean_val:.4f}   | Â±{std_val:.4f}\")\n","    print(\"-\" * 45)\n","\n","print(\"\\n[ë…¼ë¬¸ ì‘ì„± ê°€ì´ë“œ]\")\n","print(\"1. Table 2ì—ëŠ” ê° í´ë˜ìŠ¤ì˜ F1-Scoreì™€ Recallì„ ì¤‘ì‹¬ìœ¼ë¡œ ê¸°ì¬í•˜ì„¸ìš”.\")\n","print(f\"2. íŠ¹íˆ 'Fall Recall'ì´ {np.mean(metrics_storage['Falldown']['recall']):.4f}ë¡œ ë†’ë‹¤ëŠ” ì ì„ ê°•ì¡°í•˜ì„¸ìš”.\")"]}]}