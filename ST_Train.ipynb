{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPKVwJQtZn3dAKVWqZpq8xU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iXe3IWaXYStL"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sns\n","import os\n","from google.colab import drive\n","from tqdm import tqdm\n","import collections\n","\n","# === 1. ì„¤ì • ===\n","if not os.path.exists('/content/drive'):\n","    drive.mount('/content/drive')\n","\n","ROOT_PATH = '/content/drive/MyDrive/Capstone_Project'\n","DATA_PATH = os.path.join(ROOT_PATH, 'processed_data', 'train_data_30_10.npy')\n","LABEL_PATH = os.path.join(ROOT_PATH, 'processed_data', 'train_label_30_10.npy')\n","MODEL_SAVE_BASE = os.path.join(ROOT_PATH, 'experiment_full_metric')\n","\n","CLASSES = ['Walking', 'Shopping', 'Falldown', 'Threat']\n","BATCH_SIZE = 32\n","EPOCHS = 30\n","LEARNING_RATE = 0.01  # í•™ìŠµë¥  ì¡°ì • (ì•ˆì •ì„± í™•ë³´)\n","NUM_RUNS = 5\n","step_size = 15\n","\n","# === 2. ë°ì´í„° ë¡œë“œ ===\n","print(\"â–¶ ë°ì´í„° ë¡œë”©...\")\n","X = np.load(DATA_PATH)\n","Y = np.load(LABEL_PATH)\n","\n","class_weights = compute_class_weight('balanced', classes=np.unique(Y), y=Y)\n","class_weights = torch.tensor(class_weights, dtype=torch.float32)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# === 3. ëª¨ë¸ ì •ì˜ (LSTM Baseline) ===\n","# class ActionRecognitionModel(nn.Module):\n","#     def __init__(self, num_classes, input_size=34, hidden_size=128, num_layers=2): # input_size=34 (17 joints * 2 coords)\n","#         super(ActionRecognitionModel, self).__init__()\n","#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n","#         self.fc = nn.Linear(hidden_size, num_classes)\n","\n","#     def forward(self, x):\n","#         # x shape: (N, C, T, V, M) -> (32, 3, 30, 17, 1) ê°€ì •\n","#         N, C, T, V, M = x.size()\n","#         x = x.permute(0, 2, 3, 1, 4).contiguous() # (N, T, V, C, M)\n","#         x = x.view(N, T, -1)  # (N, T, V*C*M) -> Flatten features per frame\n","\n","#         lstm_out, _ = self.lstm(x)\n","#         last_output = lstm_out[:, -1, :]\n","#         return self.fc(last_output)\n","\n","class Graph:\n","    def __init__(self, num_node=17):\n","        self.num_node = num_node\n","        self.edges = [(0,1),(0,2),(1,3),(2,4),(5,7),(7,9),(6,8),(8,10),(5,6),(5,11),(6,12),(11,12),(11,13),(13,15),(12,14),(14,16)]\n","        self.A = self.get_adjacency_matrix(self.edges, self.num_node)\n","    def get_adjacency_matrix(self, edges, num_node):\n","        A = np.zeros((num_node, num_node))\n","        for i in range(num_node): A[i, i] = 1\n","        for edge in edges:\n","            A[edge[0], edge[1]] = 1\n","            A[edge[1], edge[0]] = 1\n","        row_sum = np.sum(A, axis=1)\n","        D_inv_sqrt = np.power(row_sum, -0.5)\n","        D_inv_sqrt[np.isinf(D_inv_sqrt)] = 0.\n","        D_mat = np.diag(D_inv_sqrt)\n","        return torch.tensor(D_mat.dot(A).dot(D_mat), dtype=torch.float32)\n","\n","class STGCNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, A, stride=1, residual=True):\n","        super().__init__()\n","        self.A = A\n","        self.gcn = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        self.tcn = nn.Sequential(\n","            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n","            # Kernel size (9,1)ì€ Tê°€ 9ë³´ë‹¤ ì‘ìœ¼ë©´ ì—ëŸ¬ë‚  ìˆ˜ ìˆìœ¼ë‚˜ padding=4ê°€ ìˆì–´ì„œ T=1ë„ ì²˜ë¦¬ê°€ëŠ¥\n","            nn.Conv2d(out_channels, out_channels, kernel_size=(9,1), padding=(4,0), stride=(stride,1)),\n","            nn.BatchNorm2d(out_channels), nn.Dropout(0.5, inplace=True)\n","        )\n","        if not residual: self.residual_conv = lambda x: 0\n","        elif (in_channels == out_channels) and (stride == 1): self.residual_conv = lambda x: x\n","        else: self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=(stride,1))\n","    def forward(self, x):\n","        x_spatial = self.gcn(x)\n","        N, C, T, V = x_spatial.size()\n","        x_spatial = torch.matmul(x_spatial.view(N, C*T, V), self.A.to(x.device)).view(N, C, T, V)\n","        return torch.relu(self.tcn(x_spatial) + self.residual_conv(x))\n","\n","class ActionRecognitionModel(nn.Module):\n","    def __init__(self, num_classes, in_channels=3, num_frames=30, num_joints=17):\n","        super().__init__()\n","        graph = Graph(num_node=num_joints)\n","        self.A = nn.Parameter(graph.A, requires_grad=False)\n","        self.data_bn = nn.BatchNorm1d(in_channels * num_joints)\n","\n","        # [ìˆ˜ì •ë¨ 3] ê²½ëŸ‰í™” ì±„ë„ ì ìš© (Half-STGCN)\n","        # 64-128-256 êµ¬ì¡°ë¥¼ -> 32-64-128 êµ¬ì¡°ë¡œ ë³€ê²½í•˜ì—¬ íŒŒë¼ë¯¸í„°ì™€ ì—°ì‚°ëŸ‰ ê°ì†Œ\n","        self.st_gcn_networks = nn.Sequential(\n","            STGCNBlock(in_channels, 32, self.A),  # 64 -> 32\n","            STGCNBlock(32, 32, self.A),           # 64 -> 32\n","            STGCNBlock(32, 32, self.A),           # 64 -> 32\n","            STGCNBlock(32, 64, self.A, stride=2), # 64 -> 32, 128 -> 64\n","            STGCNBlock(64, 64, self.A),           # 128 -> 64\n","            STGCNBlock(64, 64, self.A),           # 128 -> 64\n","            STGCNBlock(64, 128, self.A, stride=2),# 128 -> 64, 256 -> 128\n","            STGCNBlock(128, 128, self.A),         # 256 -> 128\n","            STGCNBlock(128, 128, self.A)          # 256 -> 128\n","        )\n","        # ë§ˆì§€ë§‰ FC Layer ì…ë ¥ ì±„ë„ë„ 128ë¡œ ê°ì†Œ\n","        self.fcn = nn.Conv2d(128, num_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","        N, C, T, V, M = x.size()\n","        x = x.mean(dim=4) # (N, C, T, V)\n","        x = x.permute(0, 1, 3, 2).contiguous().view(N, C*V, T)\n","        x = self.data_bn(x)\n","        x = x.view(N, C, V, T).permute(0, 1, 3, 2).contiguous()\n","        x = self.st_gcn_networks(x)\n","        # Global Average Pooling (T, V ì°¨ì›ì— ëŒ€í•´ í‰ê· )\n","        x = torch.nn.functional.avg_pool2d(x, x.size()[2:])\n","        return self.fcn(x).view(x.size(0), -1)\n","\n","# === 4. ê²°ê³¼ ì €ì¥ì†Œ ì´ˆê¸°í™” ===\n","metrics_storage = {cls: {'precision': [], 'recall': [], 'f1-score': []} for cls in CLASSES}\n","metrics_storage['Overall'] = {'accuracy': []}\n","\n","# ê·¸ë˜í”„ ë° í˜¼ëˆí–‰ë ¬ìš© ë°ì´í„° ì €ì¥ì†Œ\n","all_history = {'train_loss': [], 'train_acc': [], 'val_acc': []} # ì „ì²´ ì‹¤í–‰ í‰ê· ìš©\n","confusion_preds = [] # ëª¨ë“  Runì˜ ì˜ˆì¸¡ê°’ ëˆ„ì \n","confusion_labels = [] # ëª¨ë“  Runì˜ ì •ë‹µê°’ ëˆ„ì \n","\n","print(f\"\\nğŸš€ ì´ {NUM_RUNS}íšŒ ë°˜ë³µ ì‹¤í—˜ ì‹œì‘ (Train Acc í¬í•¨)\")\n","\n","for run in range(1, NUM_RUNS + 1):\n","    print(\"-\" * 60)\n","    print(f\"ğŸ”„ Run {run}/{NUM_RUNS}\")\n","\n","    # ì‹œë“œ ê³ ì • (ì¬í˜„ì„± í™•ë³´)\n","    current_seed = 42 + run\n","    torch.manual_seed(current_seed)\n","    np.random.seed(current_seed)\n","\n","    # ë°ì´í„° ë¶„í• \n","    X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=current_seed, stratify=Y)\n","\n","    # Input Size ìë™ ê³„ì‚° (Feature ìˆ˜)\n","    input_dim = X_train.shape[1] * X_train.shape[3] # C * V (ë³´í†µ 2*17=34 or 3*17=51)\n","\n","    train_data = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n","    val_data = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n","\n","    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","    # ëª¨ë¸ ì´ˆê¸°í™”\n","    model = ActionRecognitionModel(num_classes=len(CLASSES), input_size=input_dim).to(device)\n","    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1)\n","\n","    best_acc = 0.0\n","    best_model_path = f\"{MODEL_SAVE_BASE}_{run}.pth\"\n","\n","    # í˜„ì¬ Runì˜ ê¸°ë¡ ì €ì¥ìš©\n","    run_train_losses = []\n","    run_train_accs = []\n","    run_val_accs = []\n","\n","    # === í•™ìŠµ ë£¨í”„ ===\n","    for epoch in range(EPOCHS):\n","        # 1. Training\n","        model.train()\n","        running_loss = 0.0\n","        train_correct = 0\n","        train_total = 0\n","\n","        train_loop = tqdm(train_loader, desc=f\"   Epoch {epoch+1}/{EPOCHS}\", leave=False)\n","\n","        for inputs, labels in train_loop:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient Clipping ì¶”ê°€\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            # Train Acc ê³„ì‚°\n","            _, predicted = torch.max(outputs.data, 1)\n","            train_total += labels.size(0)\n","            train_correct += (predicted == labels).sum().item()\n","\n","            # ì§„í–‰ë°”ì— Lossì™€ í˜„ì¬ ë°°ì¹˜ì˜ Acc í‘œì‹œ\n","            train_loop.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{100.*train_correct/train_total:.2f}%\")\n","\n","        # Epoch ì¢…ë£Œ í›„ í‰ê·  ê³„ì‚°\n","        avg_train_loss = running_loss / len(train_loader)\n","        avg_train_acc = 100 * train_correct / train_total\n","\n","        # 2. Validation\n","        model.eval()\n","        val_correct, val_total = 0, 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                _, predicted = torch.max(outputs.data, 1)\n","                val_total += labels.size(0)\n","                val_correct += (predicted == labels).sum().item()\n","\n","        val_acc = 100 * val_correct / val_total\n","        scheduler.step()\n","\n","        # ê¸°ë¡ ì €ì¥\n","        run_train_losses.append(avg_train_loss)\n","        run_train_accs.append(avg_train_acc)\n","        run_val_accs.append(val_acc)\n","\n","        # ê²°ê³¼ ì¶œë ¥ (Train Acc ì¶”ê°€ë¨)\n","        print(f\"   [Run {run}] Ep {epoch+1:02d} | Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n","\n","        if val_acc > best_acc:\n","            best_acc = val_acc\n","            torch.save(model.state_dict(), best_model_path)\n","            print(f\"      â””â”€â”€ Best Val Updated! ({best_acc:.2f}%)\")\n","\n","    # Runë³„ ê¸°ë¡ì„ ì „ì²´ ê¸°ë¡ì— ì¶”ê°€ (ê·¸ë˜í”„ìš©)\n","    all_history['train_loss'].append(run_train_losses)\n","    all_history['train_acc'].append(run_train_accs)\n","    all_history['val_acc'].append(run_val_accs)\n","\n","    # === í‰ê°€ ë° ì €ì¥ ===\n","    print(f\"\\n   âœ… Run {run} Finished. Final Best: {best_acc:.2f}%\")\n","    model.load_state_dict(torch.load(best_model_path))\n","    model.eval()\n","\n","    all_preds, all_labels = [], []\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs = inputs.to(device)\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","\n","    # í˜¼ëˆ í–‰ë ¬ ëˆ„ì  (ì „ì²´ Run í†µí•©ìš©)\n","    confusion_preds.extend(all_preds)\n","    confusion_labels.extend(all_labels)\n","\n","    report = classification_report(all_labels, all_preds, target_names=CLASSES, output_dict=True)\n","    metrics_storage['Overall']['accuracy'].append(report['accuracy'])\n","    for cls in CLASSES:\n","            metrics_storage[cls]['precision'].append(report[cls]['precision'])\n","            metrics_storage[cls]['recall'].append(report[cls]['recall'])\n","            metrics_storage[cls]['f1-score'].append(report[cls]['f1-score'])\n","\n","    print(f\"   âœ… Run {run} Summary | Best Val: {best_acc:.2f}% | Fall Recall: {report['Falldown']['recall']:.4f}\")\n","\n","# === 5. ìµœì¢… ë¦¬í¬íŠ¸ ë° ì‹œê°í™” ===\n","print(\"\\n\" + \"=\" * 80)\n","print(f\"ğŸ“Š {NUM_RUNS}íšŒ ë°˜ë³µ ì‹¤í—˜ ì¢…í•© ê²°ê³¼ (Mean Â± Std)\")\n","print(\"=\" * 80)\n","\n","acc_mean = np.mean(metrics_storage['Overall']['accuracy']) * 100\n","acc_std = np.std(metrics_storage['Overall']['accuracy']) * 100\n","print(f\"ğŸ† Overall Accuracy: {acc_mean:.2f} Â± {acc_std:.2f}%\")\n","\n","# í´ë˜ìŠ¤ë³„ í…Œì´ë¸” ì¶œë ¥\n","header = f\"{'Class':<12} | {'Metric':<10} | {'Mean':<8} | {'Std':<8}\"\n","print(\"-\" * 45)\n","print(header)\n","print(\"-\" * 45)\n","for cls in CLASSES:\n","    for metric in ['precision', 'recall', 'f1-score']:\n","        values = metrics_storage[cls][metric]\n","        print(f\"{cls:<12} | {metric.capitalize():<10} | {np.mean(values):.4f}   | Â±{np.std(values):.4f}\")\n","    print(\"-\" * 45)\n","\n","# === 6. ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (Loss & Accuracy) ===\n","plt.figure(figsize=(12, 5))\n","\n","# Loss ê·¸ë˜í”„ (ëª¨ë“  Run í‰ê· )\n","plt.subplot(1, 2, 1)\n","mean_loss = np.mean(all_history['train_loss'], axis=0)\n","std_loss = np.std(all_history['train_loss'], axis=0)\n","epochs = range(1, EPOCHS + 1)\n","plt.plot(epochs, mean_loss, 'b-', label='Training Loss')\n","plt.fill_between(epochs, mean_loss - std_loss, mean_loss + std_loss, color='b', alpha=0.2)\n","plt.title('Training Loss per Epoch')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.grid(True)\n","\n","# Accuracy ê·¸ë˜í”„ (Train vs Val í‰ê· )\n","plt.subplot(1, 2, 2)\n","mean_train_acc = np.mean(all_history['train_acc'], axis=0)\n","mean_val_acc = np.mean(all_history['val_acc'], axis=0)\n","plt.plot(epochs, mean_train_acc, 'b-', label='Train Acc')\n","plt.plot(epochs, mean_val_acc, 'r-', label='Val Acc')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy (%)')\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","\n","# === 7. í˜¼ëˆ í–‰ë ¬ (Confusion Matrix) ì‹œê°í™” ===\n","plt.figure(figsize=(8, 6))\n","cm = confusion_matrix(confusion_labels, confusion_preds)\n","# ì •ê·œí™” (ì˜µì…˜: fmt='.2f'ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë¹„ìœ¨ë¡œ í‘œì‹œë¨)\n","# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES, yticklabels=CLASSES)\n","plt.title(f'Confusion Matrix (Accumulated {NUM_RUNS} Runs)')\n","plt.ylabel('True Label')\n","plt.xlabel('Predicted Label')\n","plt.show()\n","\n","print(\"\\nâœ… ëª¨ë“  ì‹¤í—˜ê³¼ ì‹œê°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"]}]}